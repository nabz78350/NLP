import pandas as pd
import numpy as np
import os
import random
from modelling import *
from utils import *
from tqdm import tqdm
import string
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import (
    Embedding,
    Bidirectional,
    LSTM,
    Dense,
    Dropout,
    BatchNormalization,
)
from tensorflow.keras.callbacks import LearningRateScheduler
import numpy as np
from tensorflow.keras.callbacks import EarlyStopping
import pickle
from sklearn.preprocessing import LabelEncoder


def alter_word(word):
    """
    Modifies a given word by randomly applying deletion or replacement operations, based on the word's length.

    Arguments:
    word: str - The word to be altered.

    Output:
    list - A list of words generated by applying random operations to the input word.

    Description:
    Based on the length of the word, different operations such as 'replace_one', 'replace_two', 'delete_one', or 'delete_two' are applied to randomly selected positions in the word.
    Returns a list of altered word versions.
    """
    operations = []
    if len(word) < 3:
        possible_ops = ["replace_one", "delete_one"]
    else:
        possible_ops = ["replace_two", "replace_one", "delete_one", "delete_two"]

    for op in possible_ops:
        if op == "replace_one":
            pos = random.randint(0, len(word) - 1)
            new_letter = random.choice(string.ascii_letters)
            new_word = word[:pos] + new_letter + word[pos + 1 :]
            new_word = str.lower(new_word)
            operations.append(new_word)

        elif op == "replace_two":
            pos1, pos2 = random.sample(range(len(word)), 2)
            new_letter1, new_letter2 = random.sample(string.ascii_letters, 2)
            new_word = list(word)
            new_word[pos1], new_word[pos2] = new_letter1, new_letter2

            operations.append(str.lower("".join(new_word)))

        elif op == "delete_one":
            pos = random.randint(0, len(word) - 1)
            new_word = word[:pos] + word[pos + 1 :]
            new_word = str.lower(new_word)
            operations.append(new_word)

        elif op == "delete_two":
            pos1, pos2 = random.sample(range(len(word)), 2)
            new_word = "".join(
                letter for i, letter in enumerate(word) if i not in [pos1, pos2]
            )
            new_word = str.lower(new_word)
            operations.append(new_word)

    return operations


def alter_dataframe(data_class: DataClass):
    """
    Generates a dataframe by applying word alteration operations to names that exceed a frequency threshold.

    Arguments:
    data_class: DataClass - An instance of DataClass with access to a dataset including frequency information of names.

    Output:
    pd.DataFrame - A dataframe containing altered names paired with their original forms.

    Description:
    Filters names with a frequency over 1000 from the 'freq_name' dataframe, applies the 'alter_word' function to each name,
    and collects the results in a new dataframe with columns 'X' for altered names and 'Y' for the original names.
    """
    data_model_translate = data_class.freq_name[data_class.freq_name["total"] > 1000][
        ["firstname"]
    ]
    data_model_translate["true"] = data_model_translate[["firstname"]]
    data = []
    for ope in tqdm(range(50)):
        for index in range(data_model_translate.shape[0]):
            name, true = data_model_translate.iloc[index]
            altered_name = alter_word(name)
            out = pd.DataFrame([altered_name, [true] * len(altered_name)]).T
            data.append(out)

    data_model = pd.concat(data, axis=0)
    data_model.columns = ["X", "Y"]
    return data_model


def train_model(data_model: pd.DataFrame):
    """
    Trains a machine learning model using text data from a dataframe.

    Arguments:
    data_model: pd.DataFrame - Dataframe containing input features 'X' and labels 'Y'.

    Output:
    None - Trains and saves a model, tokenizer, and label encoder to disk.

    Description:
    Encodes the labels, tokenizes and pads the texts, constructs a neural network with LSTM layers,
    compiles it, and trains it on the provided data. The model and training artifacts are saved to disk
    """
    label_encoder = LabelEncoder()
    tokenizer = Tokenizer(num_words=1000, oov_token="<OOV>", char_level=True)
    tokenizer.fit_on_texts(data_model["X"])
    X_seq = tokenizer.texts_to_sequences(data_model["X"])
    X_padded = pad_sequences(X_seq, padding="post", maxlen=8)

    Y_encoded = label_encoder.fit_transform(data_model["Y"])
    Y_categorical = to_categorical(Y_encoded)
    X_train, X_test, Y_train, Y_test = train_test_split(
        X_padded, Y_categorical, test_size=0.1, random_state=42
    )

    def scheduler(epoch, lr):
        if epoch < 10:
            return lr
        else:
            return lr * np.exp(-0.1)

    lr_schedule_callback = LearningRateScheduler(scheduler)

    model = Sequential(
        [
            Embedding(500, 32, input_length=8),  # Adjusted embedding size to 32
            Dropout(0.2),  # Dropout layer after embedding
            Bidirectional(
                LSTM(128, return_sequences=True)
            ),  # Adjusted LSTM units to 128
            BatchNormalization(),  # Batch normalization layer after first LSTM layer
            Dropout(0.2),  # Dropout layer after first LSTM Bidirectional layer
            Bidirectional(
                LSTM(128)
            ),  # Kept LSTM units to 128 for the second LSTM layer
            BatchNormalization(),  # Batch normalization layer after second LSTM layer
            Dense(128, activation="relu"),  # Adjusted Dense layer units to 128
            Dropout(0.5),  # Increased dropout rate before the final layer
            Dense(
                Y_categorical.shape[1], activation="softmax"
            ),  # Output layer remains unchanged
        ]
    )

    model.compile(
        optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"]
    )
    early_stopping = EarlyStopping(monitor="val_loss", patience=10)

    model.summary()

    output_dir = "model_misspelling"
    check_or_create_directory(output_dir)
    with open(os.path.join(output_dir, "label_encoder.pickle"), "wb") as handle:
        pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)

    with open(os.path.join(output_dir, "tokenizer.pickle"), "wb") as handle:
        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

    early_stopping = EarlyStopping(monitor="val_loss", patience=10)
    history = model.fit(
        X_train,
        Y_train,
        epochs=50,
        validation_data=(X_test, Y_test),
        validation_split=0.25,
        callbacks=[early_stopping, lr_schedule_callback],
        verbose=2,
    )
    df = pd.DataFrame(history.history)
    check_or_create_directory("model_misspelling")
    df.to_excel("model_misspelling/history.xlsx")
    model.save(os.path.join(output_dir, "my_model.h5"))


def main():
    """
    Orchestrates the workflow of data alteration and model training.

    Arguments:
    None

    Output:
    None

    Description:
    Initializes a DataClass instance, alters the dataset using 'alter_dataframe', and trains the model using 'train_model'.
    """
    data_class = DataClass(use_prediction=True, use_enhanced="no", custom=True)
    data_model = alter_dataframe(data_class)
    train_model(data_model)


if __name__ == "__main__":
    main()
